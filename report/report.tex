\documentclass[12pt]{article}

\usepackage[letterpaper, hmargin=0.75in, vmargin=0.75in]{geometry}
\usepackage{float}
\usepackage{url}
\usepackage{mathtools}

% Fill in these values to make your life easier
\newcommand{\iterations}{???}
\newcommand{\physicalcores}{4}
\newcommand{\virtualcpus}{8}

\pagestyle{empty}

\title{ECE 459: Programming for Performance\\Assignment 1}
\author{Ghanan Gowripalan}
\date{\today}

\begin{document}

\maketitle

\section*{Part 0: Resource Leak}

The resource leak was caused by the fact that the {\tt png\_struct} struct that is be pointed to by the pointer {\tt png\_ptr}, and the {\tt png\_info} struct being pointed to by {\tt info\_ptr} was not freed or destroyed with an appropriate function. I fixed this by calling the {\tt png\_destroy\_write\_struct} function which destroys both the {\tt png\_struct} and {\tt png\_info} structs when I pass in the pointer to the pointer to these structs as arguments. The problem was within the {\tt write\_png\_file} function.

\section*{Part 1: Pthreads}

My code is thread-safe because any function that is being called on by each of the spawned threads that modifies shared data (each thread can write different data) are protected by mutex locks. Other functions that are called on by threads that are not protected by mutex locks but also write to a shared space in memory will not cause race conditions because whenever there is a WAW, they write the exact same data.

There are no race conditions because each of the threads only do writes(WAW). If there are any overlapping writes, the will write the exact same data because if they are writing to the same location in the buffer, then the fragments are the same. If the fragments are the exact same, then they will have the same data. The one exception to this is the {\tt get\_url} function which is protected by a mutex lock.

I ran experiments on a Intel(R) Core(TM) i7-2600K CPU @ 3.40GHz CPU. It has \physicalcores{} physical cores and \virtualcpus{} virtual
CPUs. Tables~\ref{tbl_sequential}~and~\ref{tbl_parallel} present my results.

\begin{table}[H]
  \centering
  \begin{tabular}{lr}
    & {\bf Time (s)} \\
    \hline
    Run 1 & 34.515 \\
    Run 2 & 61.680 \\
    Run 3 & 49.144 \\
    \hline
    Average & 48.446 \\
  \end{tabular}
  \caption{\label{tbl_sequential}Sequential executions terminate in a mean of 48.446 seconds.}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{lrr}
    & {\bf N=4, Time (s)} & {\bf N=64, Time (s)} \\
    \hline
    Run 1 & 24.106 & 34.670 \\
    Run 2 & 22.890 & 34.895 \\
    Run 3 & 27.354 & 45.169 \\
    \hline
    Average & 24.783 & 38.245 \\
  \end{tabular}
  \caption{\label{tbl_parallel}Parallel executions terminate in a mean of 24.783 seconds when using 4 threads, and 38.245 seconds when using 64 threads.}
\end{table}

\section*{Part 2: Nonblocking I/O}

Table~\ref{tbl_nbio} presents results from my non-blocking I/O implementation. I started $4$ requests
simultaneously.

\begin{table}[H]
  \centering
  \begin{tabular}{lr}
    & {\bf Time (s)} \\
    \hline
    Run 1 & 25.174 \\
    Run 2 & 24.259 \\
    Run 3 & 15.575 \\
    Run 4 & 30.982 \\
    Run 5 & 27.604 \\
    Run 6 & 17.349 \\
    \hline
    Average & 23.491 \\
  \end{tabular}
  \caption{\label{tbl_nbio}Non-blocking I/O executions terminate in a mean of $23.491$ seconds.}
\end{table}

\paragraph{Discussion.} As expected, the sequential execution performed much worse when compared to the parallel and non-blocking IO version. The sequential version executed with a mean run time of 48.446s. The parallel and non-blocking IO implementations seemed to perform similarly. The parallel and non-blocking IO implementations' mean runtimes were 24.783s and 23.491s, respectively for 4 threads/concurrent connections.

It seems like even though I didn't expliclty create multiple threads for the non-blocking IO implemenations, the CURLM (multi) api created a thread for each CURL request. This became evident when using valgrind with the non-blocking IO implemention, it seemed like the CURLM (multi) api spawned multiple threads (one for each of the curls) - valgrind reported threads being spawned. This implies that in both parallelized versions, threads were created (expliclty or not).

This helps make it more clear as to why both the parallel and non-blocking IO implementations performed similarly:
\begin{itemize}
\item In both implementations (when {\tt num\_threads} was set to 4), 4 threads were spawned.
\item Each thread handled one curl request at a time.
\end{itemize}

In the parallel implemenation, the writing of the downloaded fragment to the output buffer is done in each thread, and in the non-blocking IO implemenation, they are done serially (after each curl completes). These operations are $much$ faster thant the curl operations. Also just because writing to the output buffe is done in each thread independantly, doesn't mean they execute at the same time. They may also seem to be occuring somewhat serially if the curl happens to finish after eachother instead of all at the same time. Hence, the runtime difference between the two implemenations regarding writing of the retreived fragment data to the output buffer can be considered negligible compared the CURL request times.

\section*{Part 3: Amdahl's Law and Gustafson's Law}

Let T(n) be the execution time with n threads:
\[ T(1) = 48.446s \]
\[ T(4) = 24.783s \]
\[ T(8) = 22.483s \]

Using Amdahl's Law, we know that
\[ T_p = T_s * (S + P/N) \]
where $T_p$ is the time for the program to run in parallel, $T_s$ is the time to run the program serially, $S$ is the fraction of the program that must be executed serially, $P$ is the fraction of the program that can be executed in parallel, and $N$ is the number of processors/parallel executions.

With this we can calculate the time that is spent in the serial portion.
\[ T(4) = T(1) * (S + P/4) \]
\[ S + P = 1 \]
\[ S = 1 - P \]
\[ T(4)	= T(1) * (1 - P + P/4) \]
\[ T(4) = T(1) * (1 - 3 * P/4) \]
\[ 24.783s = 48.446s * (1 - 3 * P/4) \]
\[ 24.783s/48.446s = 1 - 3 * P/4 \]
\[ 1 - 24.783s/48.446s = 3 * P/4 \]
\[ (4/3) * (1 - 24.783s/48.446s) = P \]
\[ 0.651 = P \]
\[ S = 1 - P \]
\[ S = 1 - 0.651 \]
\[ S = 0.349 \]

To confirm, do the same with T(8).
\[ T(8) = T(1) * (S + P/8) \]
\[ S + P = 1 \]
\[ S = 1 - P \]
\[ T(8) = T(1) * (1 - P + P/8) \]
\[ T(8) = T(1) * (1 - 7 * P/8) \]
\[ T(8)/T(1) = 1 - 7 * P/8 \]
\[ 1 - (T(8)/T(1)) = 7 * P/8 \]
\[ (8/7) * (1 - (T(8)/T(1))) = P \]
\[ (8/7) * (1 - (22.483s/48.446s)) = P \]
\[ 0.612 = P \]
\[ S = 1 - P \]
\[ S = 0.378 \]

We can see that the S and P values are close. With this we know the time spent executing the serial portion of the program is:
\[ T = T_s * S_{average} \]
\[ T = 48.446s * (0.378 + 0.349) / 2 \]
\[ T = 17.610 \]

Therefore, it is estimated that approximately 17.610s is spent executing the serial part.

Amdahl's Law applies because it provides a clear equation for calculating serial and parallel times given the percentage of time spent in the serial and parallel portions, along with the number of parallel executions. Alternatively, we can also use Amdahl's Law to go backwards (calculate the percentage of time spent in the serial and parallel part when given the serial and parallel execution time with some number of parallel executions - which is what we did here).

\end{document}
